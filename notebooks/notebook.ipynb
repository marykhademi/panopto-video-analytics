{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook: Panopto Course Analytics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Import: Panopto export contains multiple sheets, with two metadata sheets (course list and data definitions) and individual course data sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the excel file and examine the structure\n",
    "\n",
    "data_file = '../data/raw/df.xlsx'\n",
    "\n",
    "data = pd.read_excel(data_file, sheet_name = None)\n",
    "\n",
    "# Display the sheet namesp\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Course (sheet) Selection and Initial Data Exploration\n",
    "\n",
    "Course start from index two or sheet three. Then we will explore the course name, number of rows and columns present in the course dataset as well as the column (variable) names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a specific course (sheet) for analysis\n",
    "\n",
    "course_name = list(data.keys())[2] # courses start from index 2 (third sheet)\n",
    "\n",
    "df = pd.read_excel(data_file, sheet_name  = course_name)\n",
    "\n",
    "print(f\"Course name: '{course_name}'\\n\")\n",
    "\n",
    "print(f\"Dataset structure: {df.shape}\\n\") # structure: (number of rows, number of columns)\n",
    "\n",
    "print(f\"Column names in '{course_name}' course:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i+1}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Cleaning Stage: Standardize Column Names\n",
    "\n",
    "- provide a rationale for standardizing column names (easier to work with programmatically and follow Python naming conventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to standardize column names across different course datasets\n",
    "\n",
    "def standardize_column_names(df):\n",
    "    # create a copy of the course dataset\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # clean column names: lowercase, replace spaces with underscores, remove special characters\n",
    "    df_clean.columns = (df_clean.columns\n",
    "                        .str.lower()\n",
    "                        .str.replace(' ', '_')\n",
    "                        .str.replace('-', '_')\n",
    "                        .str.replace(' ', '')\n",
    "                        )\n",
    "    \n",
    "    return df_clean \n",
    "\n",
    "df = standardize_column_names(df)\n",
    "\n",
    "# Display the cleaned column names\n",
    "print(f\"New column names in '{course_name}' dataset:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i+1}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data Type Inspection and Conversion \n",
    "\n",
    "- what info() does\n",
    "- explain what each column represents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify data types and basic statistics\n",
    "\n",
    "print(f\"'{course_name}' course basic information:\\n\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nA summary of numerical columns in '{course_name}' course\\n\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Handling Missing Values\n",
    "\n",
    "- rationale for removing missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any missing values\n",
    "print(f\"Missing values in each column:\\n\\n{(df.isnull().sum()) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If missing values were found in previous cell, run this function to remove them\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Data Quality \n",
    "\n",
    "- understanding the distribution of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "\n",
    "print(\"Unique values in categorical columns:\")\n",
    "\n",
    "categorical_columns = ['media_type', 'creator']\n",
    "\n",
    "for i, col in enumerate(categorical_columns):\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{i+1}. '{col}' column: {df[col].nunique()} unique values\")\n",
    "        print(df[col].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
